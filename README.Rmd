---
title: "Using Random Forest for Eddy Covariance flux gap-filling"
author: "Felix Nieberding"
date: "2021-11-25"
output: rmarkdown::github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Description
This Repo contains a function and use case for gap-filling Eddy Covariance data using Random Forests algorithm (Breiman, 2001). 

The function in *\_RF_impute_missing_fluxes.R* is basically a convenient wrapper using `caret::train()` to perform gap-filling of input variables using `randomForest::randomForest()`. It is based on the code provided by [Yeonuk Kim](https://github.com/yeonukkim/EC_FCH4_gapfilling) based on his recent publication in Global Change Biology (Kim et al., 2020).

The script *gap_filling.R* imports the data and sets the processing options (e.g. mtry, n_trees, pre processing steps etc.) Then *\_RF_impute_missing_fluxes.R* estimates the train parameters for every (flux) variable to be filled on an annual basis. The script is performed in parallel to increase computation time and can easily be adapted to the number of cores available.  

The other two scripts (*check_output.R* and *get_mtry.R*) can be used to inspect model accuracies and results for mtry determination.

### Output
The function produces 4 different outputs in 3 different folders.

**./RF_models/mtry_[...].csv:** Error metrics of mtry estimation, gets only printed if train_mtry = TRUE

**./RF_models/mod_RF_[...].rds:** The RF model itself.

**./RF_results/pred_RF_[...].csv:** The results of gap-filling, i.e. the original values, the gap-filling results, the residuals and a column idicating if the values have been used for training only.

**./RF_plots/RF_[...].png:** Plots showing the results of the gap-filling and accuracy metrics.

**[...]:**  several suffixes getting concatenated indicating the variable which gets filled (_Flux), the year (_Year), the pre-processing method (_impute), the number of trees grown (_N_trees) and a freely choosable suffix (_suffix).


### Example plot
```{r echo = FALSE, message = FALSE}
library(ggplot2)
library(plotly)
result <- read.csv("~/GFZ/_Dagow/5_data_analysis/Felix/3_gap_filling/RF_results/pred_RF_H_2015_medianImpute.csv")

result$Timestamp <- as.POSIXct(result$Timestamp)

mod_RF <- readRDS("~/GFZ/_Dagow/5_data_analysis/Felix/3_gap_filling/RF_models/mod_RF_H_2015_medianImpute.rds")

test_value = result[which(result[, "test_set"] == 1), "value"]
test_pred = result[which(result[, "test_set"] == 1), "RF_pred"]
    
test_pred = test_pred[which(!is.na(test_value))]
test_value = test_value[which(!is.na(test_value))]
    
rss = sum((test_value - test_pred)^2, na.rm = T)
tss = sum((test_value - mean(test_value, na.rm = T))^2)

RMSE_test = round(caret::RMSE(test_pred, test_value), 3)
RSQUARE_test = round(1 - rss / tss, 3)
    
# out-of-bag error
RMSE_oob = round(sqrt(min(mod_RF$finalModel$mse)), 3)
RSQUARE_oob = round(max(mod_RF$finalModel$rsq), 3)
    
# plot results
error_pos.x = rep(as.POSIXct(as.numeric(max(result$Timestamp)) * 0.996, origin = "1970-01-01"), times = 4)
error_pos.y = c(max(result$RF_filled) * 0.98, max(result$RF_filled) * 0.88, max(result$RF_filled) * 0.78, max(result$RF_filled) * 0.68)
error_labels = c(paste0("RMSE_test: ", RMSE_test), paste0("RMSE_oob: ", RMSE_oob), paste0("RSQUARE_test: ", RSQUARE_test),
                 paste0("RSQUARE_oob: ", RSQUARE_oob))

p <-  ggplot(result, aes(Timestamp, RF_filled)) +
      geom_line(aes(Timestamp, RF_filled), color = "black", na.rm = T) +
      geom_point(color = "red", na.rm = T, size = .8) +
      geom_point(aes(Timestamp, value), color= "black", na.rm = T, size = .8)+
      annotate("text", 
               x = error_pos.x, 
               y = error_pos.y, 
               label = error_labels,
               hjust = "left") +
      scale_y_continuous(name = expression('H (W m'^-2*')')) +
      # scale_y_continuous(name = "H (W/m²)") +
      scale_x_datetime(date_breaks = "1 month", date_labels = "%m", name = "Month") +
      theme_bw()

p 
```


### References:
Breiman, L.: Random Forests, Machine Learning, 45, 5–32, doi:10.1023/A:1010933404324, 2001.

Kim, Y., Johnson, M. S., Knox, S. H., Black, T. A., Dalmagro, H. J., Kang, M., Kim, J., and Baldocchi, D.: Gap-filling approaches for eddy covariance methane fluxes: A comparison of three machine learning algorithms and a traditional method with principal component analysis, Global Change Biol, 26, 1499–1518, doi:10.1111/gcb.14845, 2020.
